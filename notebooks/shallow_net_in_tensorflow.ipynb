{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXe7qqFO6_a0"
   },
   "source": [
    "# Shallow Neural Network in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jmuNn2tq6_a3"
   },
   "source": [
    "Build a shallow neural network to classify handwritten digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j3kXOHzK6_a3"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jonkrohn/DLTFpT/blob/master/notebooks/shallow_net_in_tensorflow.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N3UVHR0h6_a3"
   },
   "source": [
    "#### Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "q3D0fOWA6_a3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ylXl_LZW6_a4"
   },
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_ul223LE6_a5",
    "outputId": "03320d07-8a15-4292-a225-a37093872858",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_valid, y_valid) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "emvpQ-bf6_a5",
    "outputId": "fc2cee68-938a-4369-dd52-ca96fd6fdc56"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7OxM-LzU6_a6",
    "outputId": "8230831f-83d2-4d95-ae4d-7aee4890f3df"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AjloksfO6_a6",
    "outputId": "9d4560cd-a72b-471d-981c-d36299799961"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5], dtype=uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "6ogokP4I6_a6",
    "outputId": "50951a83-cd36-4197-fadb-8fe3a6ab26f1"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "for k in range(12):\n",
    "    plt.subplot(3, 4, k+1)\n",
    "    plt.imshow(X_train[k], cmap='Greys')\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OAoj2yr46_a7",
    "outputId": "425e6252-d8dd-4bb8-ef28-bc4ef08a915a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's check the shape of the validation set as well, \n",
    "# there are 10_000 images each image of 28 * 28 pixel size.\n",
    "X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XQ0WH4-t6_a7",
    "outputId": "85c4d5cf-199c-4c48-9548-dcc629dd5ea9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validation set - just specify the number - not image so that is why second and third dimensions are empty\n",
    "y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "yIgMMMu-6_a7",
    "outputId": "5086be9b-d6da-47ab-d2a0-c11f75950856"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# input data, shown visually.\n",
    "_ = plt.imshow(X_valid[0], cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dG7hy3fG6_a8",
    "outputId": "a7a395b9-8ec6-4fbb-91a6-54e20ad56f8e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  84, 185, 159, 151,  60,  36,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0, 222, 254, 254, 254, 254, 241, 198,\n",
       "        198, 198, 198, 198, 198, 198, 198, 170,  52,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  67, 114,  72, 114, 163, 227, 254,\n",
       "        225, 254, 254, 254, 250, 229, 254, 254, 140,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  17,  66,\n",
       "         14,  67,  67,  67,  59,  21, 236, 254, 106,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,  83, 253, 209,  18,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,  22, 233, 255,  83,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0, 129, 254, 238,  44,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,  59, 249, 254,  62,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0, 133, 254, 187,   5,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   9, 205, 248,  58,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0, 126, 254, 182,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  75, 251, 240,  57,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         19, 221, 254, 166,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
       "        203, 254, 219,  35,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  38,\n",
       "        254, 254,  77,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  31, 224,\n",
       "        254, 115,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 133, 254,\n",
       "        254,  52,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  61, 242, 254,\n",
       "        254,  52,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 121, 254, 254,\n",
       "        219,  40,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 121, 254, 207,\n",
       "         18,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input data, actually how it is stored\n",
    "# you can notice majority of the values are zero - which represents white (background)\n",
    "# few values are 255 - black (represents part of the number)\n",
    "# ranges between 0 - 255 (represents part of the number)\n",
    "X_valid[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fAR385d56_a8",
    "outputId": "58cd7ddc-6cbc-4f8e-d4b9-49d653936bae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QCYuT40d6_a8"
   },
   "source": [
    "#### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6Odu_77x6_a8"
   },
   "outputs": [],
   "source": [
    "# We are converting two dimensional image to single dimension, that is why 28 * 28 = 784\n",
    "# reshaped to 784 1D array and \n",
    "# the note we are converting it to float type. You will know why it is done in next cell.\n",
    "X_train = X_train.reshape(60000, 784).astype('float32')\n",
    "X_valid = X_valid.reshape(10000, 784).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "TPz3rucK6_a9"
   },
   "outputs": [],
   "source": [
    "# Now after above cell execution, each pixel will have value of float type and ranges from 0.0 to 255.0\n",
    "# Now we are dividing each pixel by 255\n",
    "# after dividing each pixel value will range from 0.0 to 1.0\n",
    "# Why we are doing this means\n",
    "# we know ML and Deep learning model performs well if the data if of normal distribution format\n",
    "# That is the reason, we scaled the values in 0 - 1 range, so mean and standard deviation will also be close to 1\n",
    "# DL models, fit the data more accurately.\n",
    "# Refer to maths classes of normal distribution for more details.\n",
    "X_train /= 255\n",
    "X_valid /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dxLl0pCg6_a9",
    "outputId": "fd7321ad-5492-4566-e4de-52420ebb9d99"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.32941177, 0.7254902 , 0.62352943,\n",
       "       0.5921569 , 0.23529412, 0.14117648, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.87058824, 0.99607843, 0.99607843, 0.99607843, 0.99607843,\n",
       "       0.94509804, 0.7764706 , 0.7764706 , 0.7764706 , 0.7764706 ,\n",
       "       0.7764706 , 0.7764706 , 0.7764706 , 0.7764706 , 0.6666667 ,\n",
       "       0.20392157, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.2627451 , 0.44705883,\n",
       "       0.28235295, 0.44705883, 0.6392157 , 0.8901961 , 0.99607843,\n",
       "       0.88235295, 0.99607843, 0.99607843, 0.99607843, 0.98039216,\n",
       "       0.8980392 , 0.99607843, 0.99607843, 0.54901963, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.06666667, 0.25882354, 0.05490196, 0.2627451 ,\n",
       "       0.2627451 , 0.2627451 , 0.23137255, 0.08235294, 0.9254902 ,\n",
       "       0.99607843, 0.41568628, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.3254902 , 0.99215686, 0.81960785, 0.07058824,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.08627451, 0.9137255 ,\n",
       "       1.        , 0.3254902 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.5058824 , 0.99607843, 0.93333334, 0.17254902,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.23137255, 0.9764706 ,\n",
       "       0.99607843, 0.24313726, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.52156866, 0.99607843, 0.73333335, 0.01960784,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.03529412, 0.8039216 ,\n",
       "       0.972549  , 0.22745098, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.49411765, 0.99607843, 0.7137255 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.29411766, 0.9843137 ,\n",
       "       0.9411765 , 0.22352941, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.07450981, 0.8666667 , 0.99607843, 0.6509804 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.01176471, 0.79607844, 0.99607843,\n",
       "       0.85882354, 0.13725491, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.14901961, 0.99607843, 0.99607843, 0.3019608 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.12156863, 0.8784314 , 0.99607843,\n",
       "       0.4509804 , 0.00392157, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.52156866, 0.99607843, 0.99607843, 0.20392157, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.23921569, 0.9490196 , 0.99607843,\n",
       "       0.99607843, 0.20392157, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.4745098 , 0.99607843, 0.99607843, 0.85882354, 0.15686275,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.4745098 , 0.99607843,\n",
       "       0.8117647 , 0.07058824, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        ], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "tv3n3itV6_a9"
   },
   "outputs": [],
   "source": [
    "# here, we are converting the format of the output variable\n",
    "# using one-hot encoding technique\n",
    "# instead of actually storing some actual number in output eg: 1, 2, 3...9\n",
    "# we actually have a single dimensional array of size 10\n",
    "# say the output is 3 (actual not predicted) - we will set array[3] as 1.0\n",
    "# This is a better way of representing output because,\n",
    "# even in the neural network architecture of this problem, there were 10 output neurons\n",
    "# so this aligns with that design.\n",
    "n_classes = 10\n",
    "y_train = to_categorical(y_train, n_classes)\n",
    "y_valid = to_categorical(y_valid, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NYkE_jXi6_a9",
    "outputId": "1b2be686-04ed-4aa1-bbb0-c89231c8b6d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IKupjM0z6_a9"
   },
   "source": [
    "#### Design neural network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "1KkhUt-m6_a-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sshivagangeprakash/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# just high level information of Sequential model is\n",
    "# data flows in sequential way in Neural network\n",
    "# means after one layer completes execution, the other layer will start execution using previous layer output.\n",
    "model = Sequential()\n",
    "\n",
    "# hidden Layer: \n",
    "# we are adding layers here, \n",
    "# if you observe the architecture of this problem in OneNote, \n",
    "# there is a hidden layer, with 64 neurons with sigmoid as the activation function. \n",
    "# for first hidden layer, we have to specify the input shape it 784,...\n",
    "model.add(Dense(64, activation='sigmoid', input_shape=(784,)))\n",
    "\n",
    "# output Layer:\n",
    "# How we added hidden layer, similarly we have to add output layer\n",
    "# This layer consist of 10 neurons, with softmax as the activation function.\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d1__QwQt6_a-",
    "outputId": "583effe3-3e68-4cd2-c108-7b2b4872864c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)                    </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">       Param # </span>\n",
       "\n",
       " dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">50,240</span> \n",
       "\n",
       " dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                        <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " dense (\u001b[38;5;33mDense\u001b[0m)                    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                     \u001b[38;5;34m50,240\u001b[0m \n",
       "\n",
       " dense_1 (\u001b[38;5;33mDense\u001b[0m)                  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                        \u001b[38;5;34m650\u001b[0m \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">50,890</span> (198.79 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m50,890\u001b[0m (198.79 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">50,890</span> (198.79 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m50,890\u001b[0m (198.79 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this method just summarises the model information we have created.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HjaiTp-o6_a-",
    "outputId": "6248d403-06ee-4988-8815-4c84d77cd496"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50176"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(64*784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QTkzxbRA6_a-",
    "outputId": "78558e7c-b5d4-40b6-8c50-93e5c8f95d2b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50240"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(64*784)+64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jii4lIqT6_a-",
    "outputId": "d9f21530-fb53-4d40-c539-0c71cc6a8f19"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "650"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(10*64)+10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2TSjHfhi6_a-"
   },
   "source": [
    "#### Configure model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "aT02wHcd6_a-"
   },
   "outputs": [],
   "source": [
    "# in compile phase,\n",
    "# Any ML or DL algorithm, the process will be like\n",
    "# first we initialize random values (hyper parameters) and\n",
    "# try to predict output and we compare with the actual ones\n",
    "# this difference is called as Cost or loss.\n",
    "# we optimize our model parameters to reduce the cost. \n",
    "# Exactly same thing is done below. \n",
    "\n",
    "# loss - what kind of loss or cost function we are considering it is - mean Squared Error\n",
    "\n",
    "#optimizer\n",
    "# we have optimize model parameters, to reach global minima - in this case reduce cost - what strategy are we using\n",
    "# for it - in this case SGD - Stocastic Gradient Descent with learning rate - 0.01 - steps in which model should move\n",
    "\n",
    "#accuracy\n",
    "# accuracy metrics.\n",
    "model.compile(loss='mean_squared_error', optimizer=SGD(learning_rate=0.01), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hF4I2Zou6_a-"
   },
   "source": [
    "#### Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DRWqGssx6_a_",
    "outputId": "163ec0a0-bbc4-4853-8f0b-d174827944e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 840us/step - accuracy: 0.1100 - loss: 0.0939 - val_accuracy: 0.1224 - val_loss: 0.0929\n",
      "Epoch 2/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 710us/step - accuracy: 0.1199 - loss: 0.0928 - val_accuracy: 0.1332 - val_loss: 0.0922\n",
      "Epoch 3/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 724us/step - accuracy: 0.1384 - loss: 0.0922 - val_accuracy: 0.1625 - val_loss: 0.0917\n",
      "Epoch 4/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 736us/step - accuracy: 0.1685 - loss: 0.0917 - val_accuracy: 0.1938 - val_loss: 0.0912\n",
      "Epoch 5/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - accuracy: 0.1971 - loss: 0.0913 - val_accuracy: 0.2201 - val_loss: 0.0908\n",
      "Epoch 6/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 734us/step - accuracy: 0.2193 - loss: 0.0909 - val_accuracy: 0.2401 - val_loss: 0.0904\n",
      "Epoch 7/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 757us/step - accuracy: 0.2426 - loss: 0.0904 - val_accuracy: 0.2562 - val_loss: 0.0901\n",
      "Epoch 8/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - accuracy: 0.2524 - loss: 0.0902 - val_accuracy: 0.2703 - val_loss: 0.0897\n",
      "Epoch 9/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 674us/step - accuracy: 0.2692 - loss: 0.0897 - val_accuracy: 0.2849 - val_loss: 0.0893\n",
      "Epoch 10/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - accuracy: 0.2802 - loss: 0.0894 - val_accuracy: 0.2978 - val_loss: 0.0890\n",
      "Epoch 11/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664us/step - accuracy: 0.2889 - loss: 0.0891 - val_accuracy: 0.3072 - val_loss: 0.0886\n",
      "Epoch 12/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - accuracy: 0.3000 - loss: 0.0887 - val_accuracy: 0.3158 - val_loss: 0.0883\n",
      "Epoch 13/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 676us/step - accuracy: 0.3070 - loss: 0.0884 - val_accuracy: 0.3212 - val_loss: 0.0879\n",
      "Epoch 14/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - accuracy: 0.3132 - loss: 0.0880 - val_accuracy: 0.3258 - val_loss: 0.0876\n",
      "Epoch 15/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - accuracy: 0.3153 - loss: 0.0877 - val_accuracy: 0.3305 - val_loss: 0.0873\n",
      "Epoch 16/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 732us/step - accuracy: 0.3222 - loss: 0.0874 - val_accuracy: 0.3407 - val_loss: 0.0869\n",
      "Epoch 17/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - accuracy: 0.3333 - loss: 0.0870 - val_accuracy: 0.3714 - val_loss: 0.0866\n",
      "Epoch 18/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 719us/step - accuracy: 0.3745 - loss: 0.0866 - val_accuracy: 0.4123 - val_loss: 0.0862\n",
      "Epoch 19/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 745us/step - accuracy: 0.4088 - loss: 0.0863 - val_accuracy: 0.4392 - val_loss: 0.0859\n",
      "Epoch 20/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 752us/step - accuracy: 0.4256 - loss: 0.0860 - val_accuracy: 0.4483 - val_loss: 0.0856\n",
      "Epoch 21/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 744us/step - accuracy: 0.4337 - loss: 0.0857 - val_accuracy: 0.4514 - val_loss: 0.0852\n",
      "Epoch 22/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705us/step - accuracy: 0.4360 - loss: 0.0853 - val_accuracy: 0.4523 - val_loss: 0.0848\n",
      "Epoch 23/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - accuracy: 0.4369 - loss: 0.0850 - val_accuracy: 0.4529 - val_loss: 0.0845\n",
      "Epoch 24/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - accuracy: 0.4402 - loss: 0.0846 - val_accuracy: 0.4534 - val_loss: 0.0841\n",
      "Epoch 25/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 722us/step - accuracy: 0.4406 - loss: 0.0842 - val_accuracy: 0.4556 - val_loss: 0.0837\n",
      "Epoch 26/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - accuracy: 0.4360 - loss: 0.0839 - val_accuracy: 0.4554 - val_loss: 0.0834\n",
      "Epoch 27/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 602us/step - accuracy: 0.4380 - loss: 0.0836 - val_accuracy: 0.4569 - val_loss: 0.0830\n",
      "Epoch 28/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - accuracy: 0.4443 - loss: 0.0831 - val_accuracy: 0.4580 - val_loss: 0.0826\n",
      "Epoch 29/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 805us/step - accuracy: 0.4417 - loss: 0.0827 - val_accuracy: 0.4583 - val_loss: 0.0822\n",
      "Epoch 30/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 820us/step - accuracy: 0.4383 - loss: 0.0824 - val_accuracy: 0.4615 - val_loss: 0.0818\n",
      "Epoch 31/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 812us/step - accuracy: 0.4470 - loss: 0.0819 - val_accuracy: 0.4652 - val_loss: 0.0813\n",
      "Epoch 32/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - accuracy: 0.4449 - loss: 0.0815 - val_accuracy: 0.4649 - val_loss: 0.0809\n",
      "Epoch 33/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - accuracy: 0.4470 - loss: 0.0811 - val_accuracy: 0.4662 - val_loss: 0.0805\n",
      "Epoch 34/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 715us/step - accuracy: 0.4463 - loss: 0.0807 - val_accuracy: 0.4668 - val_loss: 0.0801\n",
      "Epoch 35/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665us/step - accuracy: 0.4517 - loss: 0.0802 - val_accuracy: 0.4684 - val_loss: 0.0796\n",
      "Epoch 36/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 615us/step - accuracy: 0.4556 - loss: 0.0797 - val_accuracy: 0.4700 - val_loss: 0.0792\n",
      "Epoch 37/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - accuracy: 0.4569 - loss: 0.0794 - val_accuracy: 0.4713 - val_loss: 0.0788\n",
      "Epoch 38/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664us/step - accuracy: 0.4572 - loss: 0.0790 - val_accuracy: 0.4721 - val_loss: 0.0783\n",
      "Epoch 39/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - accuracy: 0.4593 - loss: 0.0785 - val_accuracy: 0.4737 - val_loss: 0.0779\n",
      "Epoch 40/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - accuracy: 0.4627 - loss: 0.0781 - val_accuracy: 0.4748 - val_loss: 0.0774\n",
      "Epoch 41/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 673us/step - accuracy: 0.4675 - loss: 0.0776 - val_accuracy: 0.4775 - val_loss: 0.0769\n",
      "Epoch 42/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - accuracy: 0.4684 - loss: 0.0773 - val_accuracy: 0.4796 - val_loss: 0.0765\n",
      "Epoch 43/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - accuracy: 0.4725 - loss: 0.0766 - val_accuracy: 0.4822 - val_loss: 0.0760\n",
      "Epoch 44/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - accuracy: 0.4719 - loss: 0.0763 - val_accuracy: 0.4839 - val_loss: 0.0755\n",
      "Epoch 45/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 672us/step - accuracy: 0.4790 - loss: 0.0757 - val_accuracy: 0.4856 - val_loss: 0.0751\n",
      "Epoch 46/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 724us/step - accuracy: 0.4800 - loss: 0.0755 - val_accuracy: 0.4877 - val_loss: 0.0746\n",
      "Epoch 47/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - accuracy: 0.4832 - loss: 0.0749 - val_accuracy: 0.4911 - val_loss: 0.0741\n",
      "Epoch 48/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 743us/step - accuracy: 0.4843 - loss: 0.0744 - val_accuracy: 0.4950 - val_loss: 0.0737\n",
      "Epoch 49/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - accuracy: 0.4850 - loss: 0.0741 - val_accuracy: 0.4967 - val_loss: 0.0732\n",
      "Epoch 50/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - accuracy: 0.4878 - loss: 0.0736 - val_accuracy: 0.5001 - val_loss: 0.0727\n",
      "Epoch 51/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 730us/step - accuracy: 0.4962 - loss: 0.0730 - val_accuracy: 0.5049 - val_loss: 0.0723\n",
      "Epoch 52/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - accuracy: 0.4956 - loss: 0.0726 - val_accuracy: 0.5089 - val_loss: 0.0718\n",
      "Epoch 53/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - accuracy: 0.5015 - loss: 0.0722 - val_accuracy: 0.5127 - val_loss: 0.0713\n",
      "Epoch 54/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - accuracy: 0.5070 - loss: 0.0716 - val_accuracy: 0.5162 - val_loss: 0.0709\n",
      "Epoch 55/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - accuracy: 0.5102 - loss: 0.0711 - val_accuracy: 0.5194 - val_loss: 0.0704\n",
      "Epoch 56/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - accuracy: 0.5151 - loss: 0.0706 - val_accuracy: 0.5245 - val_loss: 0.0699\n",
      "Epoch 57/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - accuracy: 0.5161 - loss: 0.0703 - val_accuracy: 0.5295 - val_loss: 0.0694\n",
      "Epoch 58/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - accuracy: 0.5220 - loss: 0.0699 - val_accuracy: 0.5359 - val_loss: 0.0690\n",
      "Epoch 59/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - accuracy: 0.5328 - loss: 0.0693 - val_accuracy: 0.5417 - val_loss: 0.0685\n",
      "Epoch 60/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - accuracy: 0.5377 - loss: 0.0688 - val_accuracy: 0.5480 - val_loss: 0.0680\n",
      "Epoch 61/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - accuracy: 0.5428 - loss: 0.0684 - val_accuracy: 0.5548 - val_loss: 0.0676\n",
      "Epoch 62/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - accuracy: 0.5506 - loss: 0.0680 - val_accuracy: 0.5628 - val_loss: 0.0671\n",
      "Epoch 63/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - accuracy: 0.5536 - loss: 0.0675 - val_accuracy: 0.5712 - val_loss: 0.0666\n",
      "Epoch 64/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - accuracy: 0.5663 - loss: 0.0671 - val_accuracy: 0.5783 - val_loss: 0.0662\n",
      "Epoch 65/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - accuracy: 0.5723 - loss: 0.0668 - val_accuracy: 0.5851 - val_loss: 0.0657\n",
      "Epoch 66/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - accuracy: 0.5774 - loss: 0.0662 - val_accuracy: 0.5938 - val_loss: 0.0653\n",
      "Epoch 67/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - accuracy: 0.5859 - loss: 0.0657 - val_accuracy: 0.6009 - val_loss: 0.0648\n",
      "Epoch 68/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - accuracy: 0.5938 - loss: 0.0655 - val_accuracy: 0.6078 - val_loss: 0.0643\n",
      "Epoch 69/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718us/step - accuracy: 0.6011 - loss: 0.0648 - val_accuracy: 0.6159 - val_loss: 0.0639\n",
      "Epoch 70/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665us/step - accuracy: 0.6079 - loss: 0.0644 - val_accuracy: 0.6225 - val_loss: 0.0634\n",
      "Epoch 71/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - accuracy: 0.6156 - loss: 0.0641 - val_accuracy: 0.6294 - val_loss: 0.0630\n",
      "Epoch 72/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 757us/step - accuracy: 0.6191 - loss: 0.0637 - val_accuracy: 0.6353 - val_loss: 0.0625\n",
      "Epoch 73/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705us/step - accuracy: 0.6292 - loss: 0.0631 - val_accuracy: 0.6425 - val_loss: 0.0621\n",
      "Epoch 74/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 844us/step - accuracy: 0.6368 - loss: 0.0625 - val_accuracy: 0.6502 - val_loss: 0.0616\n",
      "Epoch 75/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 787us/step - accuracy: 0.6413 - loss: 0.0622 - val_accuracy: 0.6559 - val_loss: 0.0612\n",
      "Epoch 76/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 742us/step - accuracy: 0.6492 - loss: 0.0617 - val_accuracy: 0.6618 - val_loss: 0.0607\n",
      "Epoch 77/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - accuracy: 0.6561 - loss: 0.0612 - val_accuracy: 0.6684 - val_loss: 0.0603\n",
      "Epoch 78/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 761us/step - accuracy: 0.6585 - loss: 0.0608 - val_accuracy: 0.6739 - val_loss: 0.0598\n",
      "Epoch 79/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 719us/step - accuracy: 0.6642 - loss: 0.0604 - val_accuracy: 0.6792 - val_loss: 0.0594\n",
      "Epoch 80/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 730us/step - accuracy: 0.6704 - loss: 0.0600 - val_accuracy: 0.6844 - val_loss: 0.0589\n",
      "Epoch 81/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 719us/step - accuracy: 0.6710 - loss: 0.0595 - val_accuracy: 0.6886 - val_loss: 0.0585\n",
      "Epoch 82/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 733us/step - accuracy: 0.6770 - loss: 0.0591 - val_accuracy: 0.6932 - val_loss: 0.0580\n",
      "Epoch 83/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 721us/step - accuracy: 0.6849 - loss: 0.0584 - val_accuracy: 0.6951 - val_loss: 0.0576\n",
      "Epoch 84/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - accuracy: 0.6858 - loss: 0.0581 - val_accuracy: 0.6986 - val_loss: 0.0572\n",
      "Epoch 85/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 674us/step - accuracy: 0.6927 - loss: 0.0576 - val_accuracy: 0.7022 - val_loss: 0.0567\n",
      "Epoch 86/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - accuracy: 0.6925 - loss: 0.0574 - val_accuracy: 0.7060 - val_loss: 0.0563\n",
      "Epoch 87/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 715us/step - accuracy: 0.6979 - loss: 0.0569 - val_accuracy: 0.7094 - val_loss: 0.0559\n",
      "Epoch 88/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670us/step - accuracy: 0.7030 - loss: 0.0565 - val_accuracy: 0.7140 - val_loss: 0.0554\n",
      "Epoch 89/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 743us/step - accuracy: 0.7027 - loss: 0.0559 - val_accuracy: 0.7179 - val_loss: 0.0550\n",
      "Epoch 90/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - accuracy: 0.7064 - loss: 0.0558 - val_accuracy: 0.7219 - val_loss: 0.0546\n",
      "Epoch 91/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 727us/step - accuracy: 0.7094 - loss: 0.0553 - val_accuracy: 0.7240 - val_loss: 0.0541\n",
      "Epoch 92/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 714us/step - accuracy: 0.7081 - loss: 0.0552 - val_accuracy: 0.7266 - val_loss: 0.0537\n",
      "Epoch 93/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - accuracy: 0.7137 - loss: 0.0543 - val_accuracy: 0.7287 - val_loss: 0.0533\n",
      "Epoch 94/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - accuracy: 0.7148 - loss: 0.0541 - val_accuracy: 0.7308 - val_loss: 0.0529\n",
      "Epoch 95/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 756us/step - accuracy: 0.7232 - loss: 0.0534 - val_accuracy: 0.7328 - val_loss: 0.0525\n",
      "Epoch 96/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 719us/step - accuracy: 0.7182 - loss: 0.0532 - val_accuracy: 0.7353 - val_loss: 0.0521\n",
      "Epoch 97/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 666us/step - accuracy: 0.7229 - loss: 0.0529 - val_accuracy: 0.7381 - val_loss: 0.0517\n",
      "Epoch 98/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - accuracy: 0.7229 - loss: 0.0525 - val_accuracy: 0.7401 - val_loss: 0.0513\n",
      "Epoch 99/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 666us/step - accuracy: 0.7272 - loss: 0.0521 - val_accuracy: 0.7415 - val_loss: 0.0509\n",
      "Epoch 100/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step - accuracy: 0.7321 - loss: 0.0516 - val_accuracy: 0.7433 - val_loss: 0.0505\n",
      "Epoch 101/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 740us/step - accuracy: 0.7343 - loss: 0.0511 - val_accuracy: 0.7445 - val_loss: 0.0501\n",
      "Epoch 102/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 641us/step - accuracy: 0.7371 - loss: 0.0507 - val_accuracy: 0.7472 - val_loss: 0.0497\n",
      "Epoch 103/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - accuracy: 0.7391 - loss: 0.0504 - val_accuracy: 0.7490 - val_loss: 0.0493\n",
      "Epoch 104/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - accuracy: 0.7355 - loss: 0.0502 - val_accuracy: 0.7508 - val_loss: 0.0490\n",
      "Epoch 105/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - accuracy: 0.7388 - loss: 0.0497 - val_accuracy: 0.7529 - val_loss: 0.0486\n",
      "Epoch 106/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 624us/step - accuracy: 0.7400 - loss: 0.0494 - val_accuracy: 0.7536 - val_loss: 0.0482\n",
      "Epoch 107/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664us/step - accuracy: 0.7428 - loss: 0.0489 - val_accuracy: 0.7550 - val_loss: 0.0479\n",
      "Epoch 108/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - accuracy: 0.7434 - loss: 0.0488 - val_accuracy: 0.7557 - val_loss: 0.0475\n",
      "Epoch 109/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 676us/step - accuracy: 0.7505 - loss: 0.0480 - val_accuracy: 0.7573 - val_loss: 0.0472\n",
      "Epoch 110/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 752us/step - accuracy: 0.7472 - loss: 0.0479 - val_accuracy: 0.7587 - val_loss: 0.0468\n",
      "Epoch 111/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 715us/step - accuracy: 0.7476 - loss: 0.0477 - val_accuracy: 0.7596 - val_loss: 0.0465\n",
      "Epoch 112/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 764us/step - accuracy: 0.7516 - loss: 0.0473 - val_accuracy: 0.7611 - val_loss: 0.0461\n",
      "Epoch 113/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 758us/step - accuracy: 0.7535 - loss: 0.0468 - val_accuracy: 0.7633 - val_loss: 0.0458\n",
      "Epoch 114/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 765us/step - accuracy: 0.7552 - loss: 0.0465 - val_accuracy: 0.7653 - val_loss: 0.0455\n",
      "Epoch 115/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - accuracy: 0.7566 - loss: 0.0462 - val_accuracy: 0.7670 - val_loss: 0.0451\n",
      "Epoch 116/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - accuracy: 0.7577 - loss: 0.0459 - val_accuracy: 0.7680 - val_loss: 0.0448\n",
      "Epoch 117/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 646us/step - accuracy: 0.7605 - loss: 0.0453 - val_accuracy: 0.7691 - val_loss: 0.0445\n",
      "Epoch 118/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - accuracy: 0.7578 - loss: 0.0453 - val_accuracy: 0.7706 - val_loss: 0.0442\n",
      "Epoch 119/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664us/step - accuracy: 0.7584 - loss: 0.0450 - val_accuracy: 0.7729 - val_loss: 0.0438\n",
      "Epoch 120/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - accuracy: 0.7634 - loss: 0.0447 - val_accuracy: 0.7738 - val_loss: 0.0435\n",
      "Epoch 121/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 727us/step - accuracy: 0.7646 - loss: 0.0444 - val_accuracy: 0.7758 - val_loss: 0.0432\n",
      "Epoch 122/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 710us/step - accuracy: 0.7679 - loss: 0.0440 - val_accuracy: 0.7776 - val_loss: 0.0429\n",
      "Epoch 123/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - accuracy: 0.7688 - loss: 0.0438 - val_accuracy: 0.7792 - val_loss: 0.0426\n",
      "Epoch 124/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - accuracy: 0.7667 - loss: 0.0436 - val_accuracy: 0.7813 - val_loss: 0.0423\n",
      "Epoch 125/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 669us/step - accuracy: 0.7733 - loss: 0.0430 - val_accuracy: 0.7830 - val_loss: 0.0420\n",
      "Epoch 126/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - accuracy: 0.7720 - loss: 0.0428 - val_accuracy: 0.7849 - val_loss: 0.0418\n",
      "Epoch 127/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - accuracy: 0.7739 - loss: 0.0425 - val_accuracy: 0.7864 - val_loss: 0.0415\n",
      "Epoch 128/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - accuracy: 0.7725 - loss: 0.0426 - val_accuracy: 0.7875 - val_loss: 0.0412\n",
      "Epoch 129/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 725us/step - accuracy: 0.7799 - loss: 0.0420 - val_accuracy: 0.7890 - val_loss: 0.0409\n",
      "Epoch 130/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - accuracy: 0.7819 - loss: 0.0416 - val_accuracy: 0.7908 - val_loss: 0.0406\n",
      "Epoch 131/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 733us/step - accuracy: 0.7844 - loss: 0.0414 - val_accuracy: 0.7928 - val_loss: 0.0404\n",
      "Epoch 132/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - accuracy: 0.7808 - loss: 0.0414 - val_accuracy: 0.7943 - val_loss: 0.0401\n",
      "Epoch 133/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - accuracy: 0.7842 - loss: 0.0409 - val_accuracy: 0.7959 - val_loss: 0.0398\n",
      "Epoch 134/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664us/step - accuracy: 0.7843 - loss: 0.0408 - val_accuracy: 0.7975 - val_loss: 0.0396\n",
      "Epoch 135/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - accuracy: 0.7871 - loss: 0.0406 - val_accuracy: 0.7990 - val_loss: 0.0393\n",
      "Epoch 136/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - accuracy: 0.7892 - loss: 0.0403 - val_accuracy: 0.8004 - val_loss: 0.0391\n",
      "Epoch 137/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 728us/step - accuracy: 0.7902 - loss: 0.0399 - val_accuracy: 0.8025 - val_loss: 0.0388\n",
      "Epoch 138/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - accuracy: 0.7962 - loss: 0.0396 - val_accuracy: 0.8046 - val_loss: 0.0386\n",
      "Epoch 139/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - accuracy: 0.7946 - loss: 0.0396 - val_accuracy: 0.8063 - val_loss: 0.0383\n",
      "Epoch 140/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - accuracy: 0.7987 - loss: 0.0390 - val_accuracy: 0.8080 - val_loss: 0.0381\n",
      "Epoch 141/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 710us/step - accuracy: 0.7980 - loss: 0.0391 - val_accuracy: 0.8098 - val_loss: 0.0379\n",
      "Epoch 142/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - accuracy: 0.7991 - loss: 0.0388 - val_accuracy: 0.8111 - val_loss: 0.0376\n",
      "Epoch 143/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 728us/step - accuracy: 0.8049 - loss: 0.0384 - val_accuracy: 0.8130 - val_loss: 0.0374\n",
      "Epoch 144/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - accuracy: 0.8017 - loss: 0.0384 - val_accuracy: 0.8151 - val_loss: 0.0372\n",
      "Epoch 145/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - accuracy: 0.8081 - loss: 0.0378 - val_accuracy: 0.8169 - val_loss: 0.0369\n",
      "Epoch 146/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - accuracy: 0.8083 - loss: 0.0378 - val_accuracy: 0.8183 - val_loss: 0.0367\n",
      "Epoch 147/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - accuracy: 0.8089 - loss: 0.0376 - val_accuracy: 0.8197 - val_loss: 0.0365\n",
      "Epoch 148/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - accuracy: 0.8083 - loss: 0.0374 - val_accuracy: 0.8217 - val_loss: 0.0363\n",
      "Epoch 149/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - accuracy: 0.8087 - loss: 0.0372 - val_accuracy: 0.8226 - val_loss: 0.0360\n",
      "Epoch 150/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - accuracy: 0.8119 - loss: 0.0369 - val_accuracy: 0.8245 - val_loss: 0.0358\n",
      "Epoch 151/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - accuracy: 0.8143 - loss: 0.0369 - val_accuracy: 0.8251 - val_loss: 0.0356\n",
      "Epoch 152/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - accuracy: 0.8125 - loss: 0.0366 - val_accuracy: 0.8260 - val_loss: 0.0354\n",
      "Epoch 153/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - accuracy: 0.8144 - loss: 0.0363 - val_accuracy: 0.8274 - val_loss: 0.0352\n",
      "Epoch 154/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - accuracy: 0.8170 - loss: 0.0362 - val_accuracy: 0.8280 - val_loss: 0.0350\n",
      "Epoch 155/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - accuracy: 0.8187 - loss: 0.0359 - val_accuracy: 0.8296 - val_loss: 0.0348\n",
      "Epoch 156/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - accuracy: 0.8197 - loss: 0.0357 - val_accuracy: 0.8303 - val_loss: 0.0346\n",
      "Epoch 157/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - accuracy: 0.8196 - loss: 0.0356 - val_accuracy: 0.8311 - val_loss: 0.0344\n",
      "Epoch 158/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 735us/step - accuracy: 0.8215 - loss: 0.0354 - val_accuracy: 0.8315 - val_loss: 0.0342\n",
      "Epoch 159/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - accuracy: 0.8226 - loss: 0.0352 - val_accuracy: 0.8324 - val_loss: 0.0340\n",
      "Epoch 160/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - accuracy: 0.8229 - loss: 0.0348 - val_accuracy: 0.8341 - val_loss: 0.0338\n",
      "Epoch 161/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - accuracy: 0.8239 - loss: 0.0349 - val_accuracy: 0.8350 - val_loss: 0.0336\n",
      "Epoch 162/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - accuracy: 0.8245 - loss: 0.0348 - val_accuracy: 0.8357 - val_loss: 0.0334\n",
      "Epoch 163/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705us/step - accuracy: 0.8291 - loss: 0.0342 - val_accuracy: 0.8361 - val_loss: 0.0333\n",
      "Epoch 164/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - accuracy: 0.8241 - loss: 0.0345 - val_accuracy: 0.8370 - val_loss: 0.0331\n",
      "Epoch 165/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - accuracy: 0.8283 - loss: 0.0341 - val_accuracy: 0.8375 - val_loss: 0.0329\n",
      "Epoch 166/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - accuracy: 0.8305 - loss: 0.0337 - val_accuracy: 0.8380 - val_loss: 0.0327\n",
      "Epoch 167/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 742us/step - accuracy: 0.8313 - loss: 0.0338 - val_accuracy: 0.8386 - val_loss: 0.0326\n",
      "Epoch 168/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 753us/step - accuracy: 0.8338 - loss: 0.0333 - val_accuracy: 0.8395 - val_loss: 0.0324\n",
      "Epoch 169/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 775us/step - accuracy: 0.8338 - loss: 0.0331 - val_accuracy: 0.8405 - val_loss: 0.0322\n",
      "Epoch 170/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 810us/step - accuracy: 0.8352 - loss: 0.0329 - val_accuracy: 0.8415 - val_loss: 0.0320\n",
      "Epoch 171/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 708us/step - accuracy: 0.8333 - loss: 0.0331 - val_accuracy: 0.8423 - val_loss: 0.0319\n",
      "Epoch 172/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 735us/step - accuracy: 0.8345 - loss: 0.0330 - val_accuracy: 0.8426 - val_loss: 0.0317\n",
      "Epoch 173/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - accuracy: 0.8363 - loss: 0.0326 - val_accuracy: 0.8433 - val_loss: 0.0316\n",
      "Epoch 174/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - accuracy: 0.8375 - loss: 0.0326 - val_accuracy: 0.8445 - val_loss: 0.0314\n",
      "Epoch 175/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - accuracy: 0.8366 - loss: 0.0324 - val_accuracy: 0.8454 - val_loss: 0.0312\n",
      "Epoch 176/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 744us/step - accuracy: 0.8385 - loss: 0.0321 - val_accuracy: 0.8459 - val_loss: 0.0311\n",
      "Epoch 177/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - accuracy: 0.8383 - loss: 0.0320 - val_accuracy: 0.8467 - val_loss: 0.0309\n",
      "Epoch 178/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 713us/step - accuracy: 0.8382 - loss: 0.0319 - val_accuracy: 0.8476 - val_loss: 0.0308\n",
      "Epoch 179/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 765us/step - accuracy: 0.8385 - loss: 0.0318 - val_accuracy: 0.8480 - val_loss: 0.0306\n",
      "Epoch 180/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - accuracy: 0.8394 - loss: 0.0317 - val_accuracy: 0.8491 - val_loss: 0.0305\n",
      "Epoch 181/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - accuracy: 0.8415 - loss: 0.0314 - val_accuracy: 0.8499 - val_loss: 0.0303\n",
      "Epoch 182/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - accuracy: 0.8432 - loss: 0.0311 - val_accuracy: 0.8507 - val_loss: 0.0302\n",
      "Epoch 183/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - accuracy: 0.8416 - loss: 0.0312 - val_accuracy: 0.8514 - val_loss: 0.0300\n",
      "Epoch 184/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - accuracy: 0.8422 - loss: 0.0311 - val_accuracy: 0.8520 - val_loss: 0.0299\n",
      "Epoch 185/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - accuracy: 0.8452 - loss: 0.0308 - val_accuracy: 0.8525 - val_loss: 0.0298\n",
      "Epoch 186/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - accuracy: 0.8444 - loss: 0.0307 - val_accuracy: 0.8530 - val_loss: 0.0296\n",
      "Epoch 187/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 713us/step - accuracy: 0.8456 - loss: 0.0306 - val_accuracy: 0.8538 - val_loss: 0.0295\n",
      "Epoch 188/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - accuracy: 0.8453 - loss: 0.0305 - val_accuracy: 0.8544 - val_loss: 0.0293\n",
      "Epoch 189/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 719us/step - accuracy: 0.8468 - loss: 0.0303 - val_accuracy: 0.8553 - val_loss: 0.0292\n",
      "Epoch 190/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 720us/step - accuracy: 0.8483 - loss: 0.0303 - val_accuracy: 0.8558 - val_loss: 0.0291\n",
      "Epoch 191/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670us/step - accuracy: 0.8495 - loss: 0.0300 - val_accuracy: 0.8562 - val_loss: 0.0290\n",
      "Epoch 192/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 727us/step - accuracy: 0.8484 - loss: 0.0300 - val_accuracy: 0.8568 - val_loss: 0.0288\n",
      "Epoch 193/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 745us/step - accuracy: 0.8473 - loss: 0.0299 - val_accuracy: 0.8576 - val_loss: 0.0287\n",
      "Epoch 194/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - accuracy: 0.8499 - loss: 0.0295 - val_accuracy: 0.8582 - val_loss: 0.0286\n",
      "Epoch 195/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - accuracy: 0.8514 - loss: 0.0293 - val_accuracy: 0.8586 - val_loss: 0.0284\n",
      "Epoch 196/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 717us/step - accuracy: 0.8523 - loss: 0.0292 - val_accuracy: 0.8589 - val_loss: 0.0283\n",
      "Epoch 197/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 708us/step - accuracy: 0.8528 - loss: 0.0292 - val_accuracy: 0.8594 - val_loss: 0.0282\n",
      "Epoch 198/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - accuracy: 0.8536 - loss: 0.0290 - val_accuracy: 0.8602 - val_loss: 0.0281\n",
      "Epoch 199/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 706us/step - accuracy: 0.8532 - loss: 0.0290 - val_accuracy: 0.8609 - val_loss: 0.0280\n",
      "Epoch 200/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 671us/step - accuracy: 0.8533 - loss: 0.0288 - val_accuracy: 0.8614 - val_loss: 0.0279\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x302efa010>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we want to train our model, \n",
    "# First we want to pass our training input and training output \n",
    "# batch size - amount of input records to be processed in each iteration. \n",
    "# epochs - number of times the whole process should run - 200 means\n",
    "# from input layer to output layer for each input records - the whole process will be repeated 200 times.\n",
    "# verbose = 1, we get some visual outputs\n",
    "# specifiy valiation data so it will compare the predicted model and show accuracy\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=128, epochs=200, verbose=1, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eFFJFRyv6_a_"
   },
   "source": [
    "#### Evaluating model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mi8oSUj06_a_",
    "outputId": "4d2dd5bf-8fbf-43b9-9445-884d2b0b22bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 348us/step - accuracy: 0.8417 - loss: 0.0306\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.027851050719618797, 0.8614000082015991]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# outputs loss and accuracy of the model.\n",
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m5kN0zO56_a_"
   },
   "source": [
    "#### Performing inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "-i_2NQrm6_a_"
   },
   "outputs": [],
   "source": [
    "# this step is optional, but for our satisfaction and understanding we are doing it\n",
    "# go above and check, first input image is number 7\n",
    "# we are converting the validation set image7 to 1D array.\n",
    "valid_0 = X_valid[0].reshape(1, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1uDQLTJv6_a_",
    "outputId": "f924b84e-1c66-4420-e441-cd7e952fef42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[4.8121847e-03, 7.6901860e-04, 4.2336322e-03, 7.4341968e-03,\n",
       "        1.0613347e-02, 6.3495552e-03, 8.7288237e-04, 9.2927068e-01,\n",
       "        3.4985240e-03, 3.2146018e-02]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we are trying to predict the output for the above test data\n",
    "# output is an array, if you observe array[7] has highest value, \n",
    "# means model is predicting that number could be 7\n",
    "model.predict(valid_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "b2cxqZcL8k7C"
   },
   "outputs": [],
   "source": [
    "# instead of above cell, same prediction can be done using predict_classes() or predict.\n",
    "# model.predict_classes(valid_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9UV84XMx6_a_",
    "outputId": "c5b7e7b4-81a8-465e-f8a9-e62e3fe02f60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([7])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The predict_classes() method no longer exists in recent TensorFlow releases. \n",
    "# Instead you could use:\n",
    "import numpy as np\n",
    "np.argmax(model.predict(valid_0), axis=-1)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "shallow_net_in_tensorflow.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
